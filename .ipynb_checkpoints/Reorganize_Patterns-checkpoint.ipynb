{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03e3e3d-7fd6-409b-8a44-a5ccb8343f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tqdm\n",
    "# ! pip install natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33dbc003-c837-4b47-9c24-6faf3fad1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    " \n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca5bb58-3c26-43d0-95dd-15e6ccc37cb5",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f7e8c8f-3e39-48e3-a496-19f40930545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files(root_dir, contains=[''], extions=['']):\n",
    "    found_files = []\n",
    "    for rt_dir, dirs, files in os.walk(root_dir):\n",
    "        for ext in extions:\n",
    "            ext = ext.lower()\n",
    "            ext_len = len(ext)\n",
    "            for file in files:\n",
    "                file_ext = file[-(ext_len):]\n",
    "                # print(file)\n",
    "                file_ext = file_ext.lower()\n",
    "                if file_ext == ext:\n",
    "                    file_name = os.path.join(rt_dir, file)\n",
    "                    found_files.append(file_name)\n",
    "                    # continue                    \n",
    "                \n",
    "        for con in contains:\n",
    "            con = con.lower()\n",
    "            con_len = len(con)\n",
    "            for file in files:\n",
    "                if con in os.path.basename(file):\n",
    "                    file_name = os.path.join(rt_dir, file)\n",
    "                    found_files.append(file_name)\n",
    "    return found_files\n",
    " \n",
    "    \n",
    "def split_to_county(df, saved_path, column_name='visitor_home_cbgs', file_suffix='', mode='a'):\n",
    "    if len(file_suffix) > 0:\n",
    "            file_suffix = \"_\" +  file_suffix \n",
    "            \n",
    "    df['county_code'] = df[column_name].str.zfill(12).str[:5]\n",
    "    county_list = df['county_code'].unique()\n",
    "    \n",
    "    county_list = [c for c in county_list if c.isnumeric()]\n",
    "    county_list = sorted(county_list) \n",
    "    print(\"   len of county_list:\", len(county_list))\n",
    "\n",
    "    df_row_cnt = len(df)\n",
    "    removed_cnt = 0\n",
    "    for idx, county in tqdm(enumerate(county_list)):  # cannot use tqdm in multiprocessing!     \n",
    "        idxs = df['county_code'] == county\n",
    "        county_df = df[idxs]\n",
    "        \n",
    "        basename = f'{county}_{column_name}{file_suffix}.csv'\n",
    "        state_code = basename[:2]\n",
    "        \n",
    "        new_name = os.path.join(saved_path, state_code, county, basename)\n",
    "\n",
    "        dirname = os.path.dirname(new_name)  \n",
    "        os.makedirs(dirname, exist_ok=True)        \n",
    "        \n",
    "        county_df = county_df[[column_name, \"placekey\", \"visits\"]].sort_values(column_name)\n",
    "        county_df.to_csv(new_name, index=False, mode=mode)\n",
    "        removed_cnt += len(county_df)\n",
    "        \n",
    "        df = df[~idxs]\n",
    "\n",
    "def unfold_df_columns(df, saved_path, file_suffix, columns=['visitor_home_cbgs', 'visitor_daytime_cbgs']):    \n",
    "    for column in columns:\n",
    "        pair_list = []\n",
    "        print(f\"   Creating edges for column {column}, {len(df)} POIs...\")\n",
    "        df = df[~df[column].isna()]\n",
    "        df.progress_apply(unfold_row_dict, args=(pair_list, column), axis=1)\n",
    "        pair_list_df = pd.DataFrame(pair_list)\n",
    "        pair_list_df.columns = [\"placekey\", column, \"visits\"]\n",
    "\n",
    "        print(f\"   Created {len(pair_list_df)} edges.\")\n",
    "\n",
    "        print(f\"   Splitting edges into county level for column {column}...\")\n",
    "        os.makedirs(saved_path, exist_ok=True)\n",
    "        split_to_county(df=pair_list_df, column_name=column, saved_path = saved_path, file_suffix=file_suffix)\n",
    "        print(\"   Finish splitting edges.\")\n",
    "\n",
    "def unfold_row_dict(row, result_list, column_name='visitor_home_cbgs'):\n",
    "\n",
    "    # if 'safegraph_place_id' in row.index:\n",
    "    #     placekey = row[\"safegraph_place_id\"]\n",
    "    # print(\"column_name:\", column_name)\n",
    "    if 'placekey' in row.index:\n",
    "        placekey = row[\"placekey\"]\n",
    "    try:     \n",
    "        a_dict = json.loads(row[column_name])\n",
    "        # print(a_dict)\n",
    "        result_list += list(zip([placekey] * len(a_dict.keys()), a_dict.keys(), a_dict.values()))\n",
    "    except Exception as e: \n",
    "        print(\"Error in  unfold_row_dict(): e, row[column_name]:\", e, row[column_name])\n",
    "        \n",
    "\n",
    "\n",
    "def patterns_to_Sqlite(df, sqlite_name):\n",
    "     \n",
    "    conn = sqlite3.connect(sqlite_name)\n",
    "    curs = conn.cursor()\n",
    "    \n",
    "    columns = ','.join(df.columns)\n",
    "    columns.replace('placekey,parent_placekey', 'placekey PRIMARY KEY,parent_placekey')\n",
    "    \n",
    "    curs.execute('create table if not exists POI ' +\n",
    "                f\"({','.join(df.columns)})\")\n",
    "    df.to_sql('POI', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    sql = f'CREATE INDEX placekey_idx ON POI(placekey);'    \n",
    "    curs.execute(sql)\n",
    "    \n",
    "    conn.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5756820b-0bcc-448a-90fc-3624f599430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '06'.isnumeric()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f990a11-5df2-448f-bfd0-49451fa1b65f",
   "metadata": {},
   "source": [
    "# Target folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d2e1e79-90e5-4843-8550-9295dd11e835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: 77096\n",
      "The top 5 and bottom 5 files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\Dewey_safegraphy_download.ipynb',\n",
       " 'H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\.ipynb_checkpoints\\\\Dewey_safegraphy_download-checkpoint.ipynb',\n",
       " 'H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\2018\\\\01\\\\01\\\\CUSTOMWEATHER\\\\DAILY\\\\daily.zip',\n",
       " 'H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\2018\\\\01\\\\01\\\\CUSTOMWEATHER\\\\HOURLY\\\\hourly.zip',\n",
       " 'H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\2018\\\\01\\\\01\\\\SAFEGRAPH\\\\MP\\\\brand_info.csv',\n",
       " '...',\n",
       " 'H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\2023\\\\08\\\\21\\\\ADVAN\\\\WP\\\\patterns_weekly_000000000496.csv.gz',\n",
       " 'H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\2023\\\\08\\\\21\\\\ADVAN\\\\WP\\\\patterns_weekly_000000000497.csv.gz',\n",
       " 'H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\2023\\\\08\\\\21\\\\ADVAN\\\\WP\\\\patterns_weekly_000000000498.csv.gz',\n",
       " 'H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\2023\\\\08\\\\21\\\\ADVAN\\\\WP\\\\patterns_weekly_000000000499.csv.gz',\n",
       " 'H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\2023\\\\08\\\\21\\\\ADVAN\\\\WP\\\\visit_panel_summary.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_root_dir = r'H:\\SafeGraph_monthly_patterns_2018-2022'\n",
    "\n",
    "all_files = get_all_files(data_root_dir)\n",
    "print(f\"Found files: {len(all_files)}\")\n",
    "print(\"The top 5 and bottom 5 files:\")\n",
    "all_files[:5] + ['...'] + all_files[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2794b04-2530-4a9b-9d35-621c585c350b",
   "metadata": {},
   "source": [
    "## Filter files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf90360d-53f9-4c4c-8264-86becedfb555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found target files: 6000\n",
      "The top 5 and bottom 5 files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\2023\\\\06\\\\05\\\\ADVAN\\\\WP\\\\patterns_weekly_000000000000.csv.gz',\n",
       " 'H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\2023\\\\06\\\\05\\\\ADVAN\\\\WP\\\\patterns_weekly_000000000001.csv.gz',\n",
       " 'H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\2023\\\\06\\\\05\\\\ADVAN\\\\WP\\\\patterns_weekly_000000000002.csv.gz',\n",
       " 'H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\2023\\\\06\\\\05\\\\ADVAN\\\\WP\\\\patterns_weekly_000000000003.csv.gz',\n",
       " 'H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\2023\\\\06\\\\05\\\\ADVAN\\\\WP\\\\patterns_weekly_000000000004.csv.gz',\n",
       " '...',\n",
       " 'H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\2023\\\\08\\\\21\\\\ADVAN\\\\WP\\\\patterns_weekly_000000000495.csv.gz',\n",
       " 'H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\2023\\\\08\\\\21\\\\ADVAN\\\\WP\\\\patterns_weekly_000000000496.csv.gz',\n",
       " 'H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\2023\\\\08\\\\21\\\\ADVAN\\\\WP\\\\patterns_weekly_000000000497.csv.gz',\n",
       " 'H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\2023\\\\08\\\\21\\\\ADVAN\\\\WP\\\\patterns_weekly_000000000498.csv.gz',\n",
       " 'H:\\\\SafeGraph_monthly_patterns_2018-2022\\\\2023\\\\08\\\\21\\\\ADVAN\\\\WP\\\\patterns_weekly_000000000499.csv.gz']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_years = '2023'\n",
    "target_months = ['08', '06', '07']\n",
    "target_dataset = ['WP']\n",
    "target_names = ['patterns_weekly_']\n",
    "target_files = []\n",
    "\n",
    "for file in all_files[:]:\n",
    "    directories = file.replace(data_root_dir, '').split(os.sep)[1:]\n",
    "    # print(directories)\n",
    "    if len(directories) < 5:\n",
    "        continue\n",
    "    year = directories[0]\n",
    "    month = directories[1]\n",
    "    dataset = directories[-2]\n",
    "    basename = directories[-1]\n",
    "    if year in target_years:\n",
    "        if month in target_months:\n",
    "            if dataset in target_dataset:\n",
    "                for target_name in target_names:\n",
    "                    if target_name in basename:\n",
    "                        target_files.append(file)\n",
    "\n",
    "print(f\"Found target files: {len(target_files)}\")\n",
    "print(\"The top 5 and bottom 5 files:\")\n",
    "target_files[:5] + ['...'] + target_files[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009a49d1-4115-4c9d-b612-be0d94323ad9",
   "metadata": {},
   "source": [
    "## Find dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa1d0749-679a-4058-87d5-14c7ff9a956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_file_df = pd.DataFrame(target_files, columns=['file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccc71b37-a046-4b86-913b-629ab21bc7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...</td>\n",
       "      <td>2023</td>\n",
       "      <td>06</td>\n",
       "      <td>05</td>\n",
       "      <td>2023-06-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...</td>\n",
       "      <td>2023</td>\n",
       "      <td>06</td>\n",
       "      <td>05</td>\n",
       "      <td>2023-06-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...</td>\n",
       "      <td>2023</td>\n",
       "      <td>06</td>\n",
       "      <td>05</td>\n",
       "      <td>2023-06-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...</td>\n",
       "      <td>2023</td>\n",
       "      <td>06</td>\n",
       "      <td>05</td>\n",
       "      <td>2023-06-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...</td>\n",
       "      <td>2023</td>\n",
       "      <td>06</td>\n",
       "      <td>05</td>\n",
       "      <td>2023-06-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...</td>\n",
       "      <td>2023</td>\n",
       "      <td>08</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-08-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...</td>\n",
       "      <td>2023</td>\n",
       "      <td>08</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-08-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...</td>\n",
       "      <td>2023</td>\n",
       "      <td>08</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-08-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...</td>\n",
       "      <td>2023</td>\n",
       "      <td>08</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-08-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...</td>\n",
       "      <td>2023</td>\n",
       "      <td>08</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-08-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   file  year month day  \\\n",
       "0     H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...  2023    06  05   \n",
       "1     H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...  2023    06  05   \n",
       "2     H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...  2023    06  05   \n",
       "3     H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...  2023    06  05   \n",
       "4     H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...  2023    06  05   \n",
       "...                                                 ...   ...   ...  ..   \n",
       "5995  H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...  2023    08  21   \n",
       "5996  H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...  2023    08  21   \n",
       "5997  H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...  2023    08  21   \n",
       "5998  H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...  2023    08  21   \n",
       "5999  H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\0...  2023    08  21   \n",
       "\n",
       "            date  \n",
       "0     2023-06-05  \n",
       "1     2023-06-05  \n",
       "2     2023-06-05  \n",
       "3     2023-06-05  \n",
       "4     2023-06-05  \n",
       "...          ...  \n",
       "5995  2023-08-21  \n",
       "5996  2023-08-21  \n",
       "5997  2023-08-21  \n",
       "5998  2023-08-21  \n",
       "5999  2023-08-21  \n",
       "\n",
       "[6000 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_start_pos = 40\n",
    "target_file_df['year'] = target_file_df['file'].str[year_start_pos:year_start_pos+4]\n",
    "target_file_df['month'] = target_file_df['file'].str[year_start_pos+5:year_start_pos+7]\n",
    "target_file_df['day'] = target_file_df['file'].str[year_start_pos+8:year_start_pos+10]\n",
    "target_file_df['date'] = target_file_df['year'] + '-' + target_file_df['month'] + '-' + target_file_df['day']\n",
    "target_file_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "076076ec-1c32-4dc6-bb3e-8b282ec9013d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2023-06-05', '2023-06-12', '2023-06-19', '2023-06-26',\n",
       "       '2023-07-03', '2023-07-10', '2023-07-17', '2023-07-24',\n",
       "       '2023-07-31', '2023-08-07', '2023-08-14', '2023-08-21'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_strings = target_file_df['date'].unique()\n",
    "date_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899bddb-bd7c-4e9b-825e-1d3be491449b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing date str: 2023-06-05, 1 / 12\n",
      "    Loading CSV 500 files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4011585a7aff430091a5fbf0bcbb164a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processing 1 / 500 files. Date range: 2023-06-05 - 2023-06-12, H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\06\\05\\ADVAN\\WP\\patterns_weekly_000000000000.csv.gz\n",
      "   Saving POI files to: K:\\SafeGraph\\Weekly_county_files\\POI_only\\POI_2023-06-05_To_2023-06-12.db\n",
      "   Creating edges for column visitor_home_cbgs, 29150 POIs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283fa6c9a9b340ad8aa3f979475a780f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14089 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Created 245503 edges.\n",
      "   Splitting edges into county level...\n",
      "   len of county_list: 3016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e3c88ddbb34c17a69f04a1a713b8da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Finish splitting edges.\n",
      "   Creating edges for column visitor_daytime_cbgs, 14089 POIs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40474fa5adaf4cc0ab5258fa65680da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13967 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Created 233527 edges.\n",
      "   Splitting edges into county level...\n",
      "   len of county_list: 2978\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35da0a8253b548b1b7f861198d52e1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Finish splitting edges.\n",
      "\n",
      "   Processing 2 / 500 files. Date range: 2023-06-05 - 2023-06-12, H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\06\\05\\ADVAN\\WP\\patterns_weekly_000000000001.csv.gz\n",
      "   Saving POI files to: K:\\SafeGraph\\Weekly_county_files\\POI_only\\POI_2023-06-05_To_2023-06-12.db\n",
      "   Creating edges for column visitor_home_cbgs, 28866 POIs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc3d85a73cb43a79544b76b9772e3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Created 246684 edges.\n",
      "   Splitting edges into county level...\n",
      "   len of county_list: 3022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad7d87911fa4a928b509117619fda87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "saved_path = r'K:\\SafeGraph\\Weekly_county_files'\n",
    "os.makedirs(saved_path, exist_ok=True)\n",
    "\n",
    "for date_idx, date in enumerate(date_strings[:]):\n",
    "    print(f'Processing date str: {date}, {date_idx + 1} / {len(date_strings)}')\n",
    "    date_df = target_file_df.query(f\"date == '{date}' \")\n",
    "    date_csv_files = date_df['file'].to_list()\n",
    "\n",
    "    unfold_columns=['visitor_home_cbgs', 'visitor_daytime_cbgs']\n",
    "      \n",
    "    print(f\"    Loading CSV {len(date_csv_files)} files...\")\n",
    "   \n",
    "    # read all patterns files\n",
    "    # weekly_df = pd.concat([pd.read_csv(f) for f in tqdm(date_csv_files[:])]) \n",
    "    for idx, f in tqdm(enumerate(date_csv_files[:])):\n",
    "        weekly_df = pd.read_csv(f)\n",
    "        # clean data\n",
    "        weekly_df = weekly_df[~weekly_df['date_range_start'].isna()]\n",
    "        weekly_df = weekly_df[~weekly_df['date_range_end'].isna()] \n",
    "        start_date = weekly_df['date_range_start'].min()[:10] # E.g.: 2018-01-15T00:00:00-09:00\n",
    "        end_date = weekly_df['date_range_end'].max()[:10]\n",
    "     \n",
    "        # print(f\"   Read {len(date_df)} files. Date range: {start_date} - {end_date}\")\n",
    "        print(f\"   Processing {idx + 1} / {len(date_df)} files. Date range: {start_date} - {end_date}, {f}\")\n",
    "        file_suffix = f\"{start_date}_To_{end_date}\"\n",
    "    \n",
    "        # Save POI CSV without the split columns.\n",
    "        POI_new_name = os.path.join(saved_path, \"POI_only\", f\"POI_{file_suffix}.db\")\n",
    "        os.makedirs(os.path.dirname(POI_new_name), exist_ok=True)\n",
    "        print(f\"   Saving POI files to: {POI_new_name}\")\n",
    "        # POI_drop_columns = ['visitor_home_cbgs', 'visitor_daytime_cbgs']\n",
    "        POI_drop_columns = unfold_columns\n",
    "        # weekly_df.drop(columns=POI_drop_columns).to_csv(POI_new_name, index=False)\n",
    "        \n",
    "        # print(\"    Writing Sqlite database...\")\n",
    "        # patterns_to_Sqlite(weekly_df.drop(columns=POI_drop_columns), POI_new_name)\n",
    "    \n",
    "        weekly_df = weekly_df[unfold_columns + ['placekey']]\n",
    "        \n",
    "        # Unfold_columns    \n",
    "        \n",
    "        unfold_df_columns(df=weekly_df, saved_path=saved_path, file_suffix=file_suffix, columns=unfold_columns)                  \n",
    "        \n",
    "        \n",
    "        print()\n",
    "\n",
    "weekly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f53c92c-5701-455b-ac63-9cb25e3ef56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['visitor_home_cbgs', 'visitor_daytime_cbgs', 'placekey'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekly_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c17dba9-afe8-447a-a604-0c1573be9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weekly_csv_dir = r'J:\\weekly_patterns_20211211\\to_cluster2'\n",
    "weekly_csv_files = get_all_files(root_dir=weekly_csv_dir, contains=['row'], extions=[''])\n",
    "weekly_csv_files = natsorted(weekly_csv_files, reverse=True)\n",
    "print(f\"Found {len(weekly_csv_files)} files. \")        \n",
    "weekly_csv_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98fc068-b57e-4571-be00-47af5712c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get week strings\n",
    "week_strings = [os.path.basename(f)[:10] for f in weekly_csv_files]\n",
    "week_strings = list(set(week_strings))\n",
    "week_strings = natsorted(week_strings, reverse=True)\n",
    "print(f\"Found {len(week_strings)} weeks.\", week_strings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e701868a-da0a-418f-a957-72e8e8f42e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# week_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9242171f-5333-44a4-b729-e44225e8a95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start to process each week\n",
    "print(\"Start to process each week:\\n\")\n",
    "saved_path = r'J:\\Safegraph\\weekly_county_files\\weekly_patterns_2018_2021'\n",
    "\n",
    "\n",
    "\n",
    "for week_idx, week_str in enumerate(week_strings[:]):\n",
    "    print(f'Processing week_str: {week_str}, {week_idx + 1} / {len(week_strings)}')\n",
    "    df_list = []\n",
    "    \n",
    "    print(f\"   Reading CSV files...\")\n",
    "    for f in weekly_csv_files:        \n",
    "        if os.path.basename(f).startswith(week_str):\n",
    "            # print(f)\n",
    "            df = pd.read_csv(f)\n",
    "            df_list.append(df)\n",
    "        # break\n",
    "            \n",
    "    weekly_df = pd.concat(df_list).iloc[:]\n",
    "    start_date = weekly_df['date_range_start'].min()[:10] # E.g.: 2018-01-15T00:00:00-09:00\n",
    "    end_date = weekly_df['date_range_end'].max()[:10]\n",
    "    print(f\"   Read {len(df_list)} files. Date range: {start_date} - {end_date}\")\n",
    "    file_suffix = f\"{start_date}_To_{end_date}\"\n",
    "\n",
    "    # Unfold_columns    \n",
    "    unfold_columns=['visitor_home_cbgs', 'visitor_daytime_cbgs']\n",
    "    unfold_df_columns(df=weekly_df, saved_path=saved_path, file_suffix=file_suffix, columns=unfold_columns)                  \n",
    "    \n",
    "    # Save POI CSV without the split columns.\n",
    "    POI_new_name = os.path.join(saved_path, \"POI\", f\"POI_{file_suffix}.csv\")\n",
    "    os.makedirs(os.path.dirname(POI_new_name), exist_ok=True)\n",
    "    print(f\"   Saving POI files to: {POI_new_name}\")\n",
    "    # POI_drop_columns = ['visitor_home_cbgs', 'visitor_daytime_cbgs']\n",
    "    POI_drop_columns = unfold_columns\n",
    "    weekly_df.drop(columns=POI_drop_columns).to_csv(POI_new_name, index=False)\n",
    "    print()\n",
    "\n",
    "print(\"All weeks were done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ea5db-6337-4e91-b833-cfd19c820c6a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.loads(weekly_df.iloc[8]['visitor_home_cbgs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72898ae1-2e9f-41d1-8533-7d3ee2ac37b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = json.loads(weekly_df.iloc[8]['visitor_home_cbgs'])\n",
    "pd.DataFrame.from_dict(dict_, orient='index', columns=[ 'visits']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74c2362-10d8-4d40-9cd0-345dd452d4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbba4eb-03c0-4972-ac90-dfa1371ceff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_list = []\n",
    "\n",
    "weekly_df.apply(unfold_row_dict, args=(pair_list,), axis=1)\n",
    "\n",
    "pair_list_df = pd.DataFrame(pair_list)\n",
    "pair_list_df.columns = [\"placekey\", \"visitor_home_cbgs\", \"visits\"]\n",
    "pair_list_df        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884318ff-3294-4662-ab42-90551a6b26f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_list_df.columns = [\"placekey\", \"visitor_home_cbgs\", \"visits\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776308b0-d7ce-4b14-895f-d981e3aa0414",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_to_county(df, saved_path):\n",
    "    df['county_code'] = df['visitor_home_cbgs'].str.zfill(12).str[:5]\n",
    "    county_list = df['county_code'].unique()\n",
    "\n",
    "    print(\"len of county_list:\", len(county_list))\n",
    "\n",
    "    df_row_cnt = len(df)\n",
    "\n",
    "    removed_cnt = 0\n",
    "\n",
    "    for idx, county in enumerate(county_list):  # cannot use tqdm in multiprocessing!\n",
    "        print(idx, county)\n",
    "        idxs = df['county_code'] == county\n",
    "        county_df = df[idxs]\n",
    "        \n",
    "        #\n",
    "\n",
    "        # new_name = f'County_{county}_{basename}'\n",
    "        basename = f'{county}.csv'\n",
    "        state_code = basename[:2]\n",
    "        \n",
    "        print(\"basename:\", basename)\n",
    "        \n",
    "        new_name = os.path.join(saved_path, state_code, basename)\n",
    "        # print(\"new_name:\", new_name)\n",
    "        # start_date = county_df[''].min()\n",
    "        \n",
    "        dirname = os.path.dirname(new_name)  \n",
    "        os.makedirs(dirname, exist_ok=True)\n",
    "        \n",
    "        \n",
    "        county_df.to_csv(new_name, index=False)\n",
    "        # print(\"len of county_df:\", len(county_df))\n",
    "\n",
    "        removed_cnt += len(county_df)\n",
    "\n",
    "        df = df[~idxs]\n",
    "\n",
    "split_to_county(df=pair_list_df, saved_path = r'J:\\Safegraph\\weekly_county_files\\weekly_patterns_20211211')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e15db-3c52-4045-aff3-38d6c16c0987",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_list_df['county_code'] = pair_list_df['visitor_home_cbgs'].str.zfill(12).str[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f63ca-425c-48e1-a0b7-76c4c58125fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "groups = pair_list_df.groupby('county_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b295f4-cd56-47a9-8717-adee2594f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups = df.groupby('county_code', as_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be65a5-06a6-4c06-83bb-1e10e430deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c6b7a9-93f2-40b4-aa07-344c322bc105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_county(df, saved_path, column_name='visitor_home_cbgs', file_suffix=''):\n",
    "    if len(file_suffix) > 0:\n",
    "            file_suffix = \"_\" +  file_suffix \n",
    "            \n",
    "    df['county_code'] = df[column_name].str.zfill(12).str[:5]\n",
    "    county_list = df['county_code'].unique()\n",
    "    \n",
    "    county_list = [c for c in county_list if c.isnumeric()]\n",
    "    county_list = sorted(county_list) \n",
    "    \n",
    "\n",
    "    df_row_cnt = len(df)\n",
    "    removed_cnt = 0\n",
    "    \n",
    "    groups = df.groupby('county_code', as_index=False)\n",
    "    # print(\"   len of county_list:\", len(county_list))\n",
    "    processed_county_cnt = 0\n",
    "    for county, county_df in groups:\n",
    "        \n",
    "        if not county.isnumeric():\n",
    "            continue\n",
    "        \n",
    "        basename = f'{county}_{column_name}{file_suffix}.csv'\n",
    "        state_code = basename[:2]\n",
    "        \n",
    "        new_name = os.path.join(saved_path, state_code, county, basename)\n",
    "\n",
    "        dirname = os.path.dirname(new_name)  \n",
    "        os.makedirs(dirname, exist_ok=True)        \n",
    "        \n",
    "        county_df = county_df[[column_name, \"placekey\", \"visits\"]].sort_values([column_name, 'visits'], ascending=[True, False])\n",
    "        county_df.to_csv(new_name, index=False)\n",
    "        removed_cnt += len(county_df)\n",
    "\n",
    "        processed_county_cnt += 1\n",
    "        if processed_county_cnt % 100 == 0:\n",
    "            print(f\"   PID {os.getpid()} finished {processed_county_cnt} / {len(groups)} counties for in period: {file_suffix}\\n\")\n",
    "            \n",
    "\n",
    "            \n",
    "split_to_county(df, \n",
    "                saved_path=, \n",
    "                column_name='visitor_home_cbgs', \n",
    "                file_suffix='test'):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305502e6-d4d1-4871-92ee-b21fa02f724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test groupby\n",
    "\n",
    "weekly_df.groupby('county_code')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
