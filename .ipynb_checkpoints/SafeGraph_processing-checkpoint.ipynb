{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e99bcec7-6cdc-43a3-adf3-08ab65c42942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install ipfn\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7ff2efb-80e9-4ac4-95ed-af9a1845db5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 64.55852549  46.23247384  35.38430991   3.82473358]\n",
      " [ 49.96791318  68.15934981 156.4985403   25.37417955]\n",
      " [ 56.72193658 144.42821667 145.0824759   53.76734831]\n",
      " [ 28.75162474  41.17995969  63.03467389  17.03373857]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipfn import ipfn\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "m = [[40, 30, 20, 10], [35, 50, 100, 75], [30, 80, 70, 120], [20, 30, 40, 50]]\n",
    "# m = [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]\n",
    "\n",
    "m = np.array(m)\n",
    "xip = np.array([150, 300, 400, 150])\n",
    "xpj = np.array([200, 300, 400, 100])\n",
    "\n",
    "aggregates = [xip, xpj]\n",
    "dimensions = [[0], [1]]\n",
    "\n",
    "IPF = ipfn.ipfn(m, aggregates, dimensions, convergence_rate=1e-6)\n",
    "m = IPF.iteration()\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e735303-a69c-4c9a-9a6b-6798d531b4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([200., 300., 400., 100.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20c24331-d9bb-4325-8790-fe27270c75f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSafeGraph_monthly_patterns_2018-2022\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2023\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m06\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m12\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mADVAN\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mWP\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpatterns_weekly_000000000490.csv.gz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSafeGraph_monthly_patterns_2018-2022\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2023\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m06\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m01\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mADVAN\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mNP\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mneighborhood_patterns_000000000128.csv.gz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv(r'G:\\SafeGraph_monthly_patterns_2018-2022\\2023\\06\\12\\ADVAN\\WP\\patterns_weekly_000000000490.csv.gz')\n",
    "df = pd.read_csv(r'H:\\SafeGraph_monthly_patterns_2018-2022\\2023\\06\\01\\ADVAN\\NP\\neighborhood_patterns_000000000128.csv.gz')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9233a46e-7088-46e8-98fa-9ab492ede73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df['popularity_by_hour'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "44196ffa-e11c-4c1b-813f-0146e23d6697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_visit_counts</th>\n",
       "      <th>normalized_visits_by_state_scaling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.0</td>\n",
       "      <td>259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29123</th>\n",
       "      <td>113.0</td>\n",
       "      <td>1259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29124</th>\n",
       "      <td>45.0</td>\n",
       "      <td>504.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29125</th>\n",
       "      <td>46.0</td>\n",
       "      <td>509.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29126</th>\n",
       "      <td>17.0</td>\n",
       "      <td>190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29127</th>\n",
       "      <td>45.0</td>\n",
       "      <td>506.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29128 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       raw_visit_counts  normalized_visits_by_state_scaling\n",
       "0                   NaN                                 NaN\n",
       "1                   NaN                                 NaN\n",
       "2                   NaN                                 NaN\n",
       "3                   NaN                                 NaN\n",
       "4                  23.0                               259.0\n",
       "...                 ...                                 ...\n",
       "29123             113.0                              1259.0\n",
       "29124              45.0                               504.0\n",
       "29125              46.0                               509.0\n",
       "29126              17.0                               190.0\n",
       "29127              45.0                               506.0\n",
       "\n",
       "[29128 rows x 2 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['raw_visit_counts', 'normalized_visits_by_state_scaling']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "144208fd-5bf2-4e5a-ab18-c3d9cff7fc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              NaN\n",
       "1              NaN\n",
       "2              NaN\n",
       "3              NaN\n",
       "4        11.260870\n",
       "           ...    \n",
       "29123    11.141593\n",
       "29124    11.200000\n",
       "29125    11.065217\n",
       "29126    11.176471\n",
       "29127    11.244444\n",
       "Length: 29128, dtype: float64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['normalized_visits_by_state_scaling'] / df['raw_visit_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f2463275-69f4-4cc8-a33f-eaae6e731700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             NaN\n",
       "1             NaN\n",
       "2             NaN\n",
       "3             NaN\n",
       "4        0.000061\n",
       "           ...   \n",
       "29123    0.000052\n",
       "29124    0.000305\n",
       "29125    0.000207\n",
       "29126    0.017528\n",
       "29127    0.000318\n",
       "Length: 29128, dtype: float64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['normalized_visits_by_region_naics_visits'] / df['raw_visit_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3d943b52-08ab-4af5-826a-687cfbdf8df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['placekey', 'parent_placekey', 'safegraph_brand_ids', 'location_name',\n",
       "       'brands', 'store_id', 'top_category', 'sub_category', 'naics_code',\n",
       "       'latitude', 'longitude', 'street_address', 'city', 'region',\n",
       "       'postal_code', 'open_hours', 'category_tags', 'opened_on', 'closed_on',\n",
       "       'tracking_closed_since', 'domains', 'geometry_type', 'polygon_wkt',\n",
       "       'polygon_class', 'enclosed', 'phone_number', 'is_synthetic',\n",
       "       'includes_parking_lot', 'iso_country_code', 'wkt_area_sq_meters',\n",
       "       'date_range_start', 'date_range_end', 'raw_visit_counts',\n",
       "       'raw_visitor_counts', 'visits_by_day', 'visits_by_each_hour', 'poi_cbg',\n",
       "       'visitor_home_cbgs', 'visitor_home_aggregation', 'visitor_daytime_cbgs',\n",
       "       'visitor_country_of_origin', 'distance_from_home', 'median_dwell',\n",
       "       'bucketed_dwell_times', 'related_same_day_brand',\n",
       "       'related_same_week_brand', 'device_type',\n",
       "       'normalized_visits_by_state_scaling',\n",
       "       'normalized_visits_by_region_naics_visits',\n",
       "       'normalized_visits_by_region_naics_visitors',\n",
       "       'normalized_visits_by_total_visits',\n",
       "       'normalized_visits_by_total_visitors'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "dccac756-e2f3-4cb7-8710-c23100bf492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_df = df\n",
    "\n",
    "def get_all_files(root_dir, contains=[''], extions=['']):\n",
    "    found_files = []\n",
    "    for rt_dir, dirs, files in os.walk(root_dir):\n",
    "        for ext in extions:\n",
    "            ext = ext.lower()\n",
    "            ext_len = len(ext)\n",
    "            for file in files:\n",
    "                file_ext = file[-(ext_len):]\n",
    "                # print(file)\n",
    "                file_ext = file_ext.lower()\n",
    "                if file_ext == ext:\n",
    "                    file_name = os.path.join(rt_dir, file)\n",
    "                    found_files.append(file_name)\n",
    "                    # continue                    \n",
    "                \n",
    "        for con in contains:\n",
    "            con = con.lower()\n",
    "            con_len = len(con)\n",
    "            for file in files:\n",
    "                if con in os.path.basename(file):\n",
    "                    file_name = os.path.join(rt_dir, file)\n",
    "                    found_files.append(file_name)\n",
    "    return found_files\n",
    "\n",
    "\n",
    "    \n",
    "def split_to_county(df, saved_path, column_name='visitor_home_cbgs', file_suffix=''):\n",
    "    if len(file_suffix) > 0:\n",
    "            file_suffix = \"_\" +  file_suffix \n",
    "            \n",
    "    df['county_code'] = df[column_name].str.zfill(12).str[:5]\n",
    "    county_list = df['county_code'].unique()\n",
    "    \n",
    "    county_list = [c for c in county_list if c.isnumeric()]\n",
    "    county_list = sorted(county_list) \n",
    "    print(\"   len of county_list:\", len(county_list))\n",
    "\n",
    "    df_row_cnt = len(df)\n",
    "    removed_cnt = 0\n",
    "    for idx, county in enumerate(county_list):  # cannot use tqdm in multiprocessing!     \n",
    "        idxs = df['county_code'] == county\n",
    "        county_df = df[idxs]\n",
    "        \n",
    "        basename = f'{county}_{column_name}{file_suffix}.csv'\n",
    "        state_code = basename[:2]\n",
    "        \n",
    "        new_name = os.path.join(saved_path, state_code, county, basename)\n",
    "\n",
    "        dirname = os.path.dirname(new_name)  \n",
    "        os.makedirs(dirname, exist_ok=True)        \n",
    "        \n",
    "        county_df = county_df[[column_name, \"placekey\", \"visits\"]].sort_values(column_name)\n",
    "        county_df.to_csv(new_name, index=False)\n",
    "        removed_cnt += len(county_df)\n",
    "        \n",
    "        df = df[~idxs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f49e1b74-d1a1-4dc5-8ed5-11519205e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sqlite3\n",
    "\n",
    "saved_path = r'D:\\Temp'\n",
    "weekly_df = df\n",
    "\n",
    "# clean data\n",
    "weekly_df = weekly_df[~weekly_df['date_range_start'].isna()]\n",
    "weekly_df = weekly_df[~weekly_df['date_range_end'].isna()] \n",
    "start_date = weekly_df['date_range_start'].min()[:10] # E.g.: 2018-01-15T00:00:00-09:00\n",
    "end_date = weekly_df['date_range_end'].max()[:10]\n",
    " \n",
    "\n",
    "# print(f\"   Read {len(df_list)} files. Date range: {start_date} - {end_date}\")\n",
    "file_suffix = f\"{start_date}_To_{end_date}\"\n",
    "\n",
    "def patterns_to_Sqlite(df, sqlite_name):\n",
    "     \n",
    "    conn = sqlite3.connect(sqlite_name)\n",
    "    curs = conn.cursor()\n",
    "    \n",
    "    columns = ','.join(df.columns)\n",
    "    columns.replace('placekey,parent_placekey', 'placekey PRIMARY KEY,parent_placekey')\n",
    "    \n",
    "    curs.execute('create table if not exists POI ' +\n",
    "                f\"({','.join(df.columns)})\")\n",
    "    df.to_sql('POI', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    sql = f'CREATE INDEX placekey_idx ON POI(placekey);'    \n",
    "    curs.execute(sql)\n",
    "    \n",
    "    conn.close\n",
    "\n",
    "sqlite_saved_path = r'D:\\Temp\\weekly_patters_Sqlite'\n",
    " \n",
    "basename = f'{file_suffix}_weekly_patterns_POI_only.db'\n",
    "\n",
    "sqlite_name = os.path.join(sqlite_saved_path, basename)\n",
    "sqlite_name\n",
    "patterns_to_Sqlite(df.drop(columns=unfold_columns), sqlite_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4ff1e6dd-bf1e-4050-8c40-bc8434e79256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Creating edges for 328799 POIs...\n",
      "   Created 2168699 edges.\n",
      "   Splitting edges into county level...\n",
      "   len of county_list: 3218\n",
      "   Finish splitting edges.\n",
      "   Creating edges for 328799 POIs...\n",
      "   Created 2148817 edges.\n",
      "   Splitting edges into county level...\n",
      "   len of county_list: 3216\n",
      "   Finish splitting edges.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# start_date = '2022-06-06'  \n",
    "# end_date = datetime.strptime(start_date, '%Y-%m-%d') + timedelta(days=7)\n",
    "# end_date = datetime.strftime(end_date, '%Y-%m-%d')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def unfold_df_columns(df, saved_path, file_suffix, columns=['visitor_home_cbgs', 'visitor_daytime_cbgs']):    \n",
    "    for column in columns:\n",
    "        pair_list = []\n",
    "        print(f\"   Creating edges for {len(df)} POIs...\")\n",
    "        df = df[~df[column].isna()]\n",
    "        df.apply(unfold_row_dict, args=(pair_list, column), axis=1)\n",
    "        pair_list_df = pd.DataFrame(pair_list)\n",
    "        pair_list_df.columns = [\"placekey\", column, \"visits\"]\n",
    "\n",
    "        print(f\"   Created {len(pair_list_df)} edges.\")\n",
    "\n",
    "        print(\"   Splitting edges into county level...\")\n",
    "        os.makedirs(saved_path, exist_ok=True)\n",
    "        split_to_county(df=pair_list_df, column_name=column, saved_path = saved_path, file_suffix=file_suffix)\n",
    "        print(\"   Finish splitting edges.\")\n",
    "\n",
    "def unfold_row_dict(row, result_list, column_name='visitor_home_cbgs'):\n",
    "\n",
    "    # if 'safegraph_place_id' in row.index:\n",
    "    #     placekey = row[\"safegraph_place_id\"]\n",
    "    # print(\"column_name:\", column_name)\n",
    "    if 'placekey' in row.index:\n",
    "        placekey = row[\"placekey\"]\n",
    "    try:     \n",
    "        a_dict = json.loads(row[column_name])\n",
    "        # print(a_dict)\n",
    "        result_list += list(zip([placekey] * len(a_dict.keys()), a_dict.keys(), a_dict.values()))\n",
    "    except Exception as e: \n",
    "        print(\"Error in  unfold_row_dict(): e, row[column_name]:\", e, row[column_name])\n",
    "        \n",
    "\n",
    "# Unfold_columns    \n",
    "unfold_columns=['visitor_home_cbgs', 'visitor_daytime_cbgs']\n",
    "unfold_df_columns(df=weekly_df, saved_path=saved_path, file_suffix=file_suffix, columns=unfold_columns)                  \n",
    "\n",
    "print(\"Done\")\n",
    "# # Save POI CSV without the split columns.\n",
    "# POI_new_name = os.path.join(saved_path, \"POI\", f\"POI_{file_suffix}.csv\")\n",
    "# os.makedirs(os.path.dirname(POI_new_name), exist_ok=True)\n",
    "# print(f\"   Saving POI files to: {POI_new_name}\")\n",
    "# # POI_drop_columns = ['visitor_home_cbgs', 'visitor_daytime_cbgs']\n",
    "# POI_drop_columns = unfold_columns\n",
    "# weekly_df.drop(columns=POI_drop_columns).to_csv(POI_new_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7316b3b3-b0bb-443f-8c1f-3a6f4b14d332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-06-06_To_2022-06-13'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_suffix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
