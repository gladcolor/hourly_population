{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "\n",
    "un = \"hning@email.sc.edu\" # Set username\n",
    "pw = \"Nhwwdewey2022,\" # Set password\n",
    "\n",
    "credentials = f\"{un}:{pw}\" # Format credentials according to the API's expectations\n",
    "print(credentials)\n",
    "\n",
    "credentials_bytes = credentials.encode('ascii')\n",
    "base64_credentials_bytes = base64.b64encode(credentials_bytes)\n",
    "base64_credentials = base64_credentials_bytes.decode('ascii')\n",
    "print(base64_credentials)\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Authorization': f'Basic {base64_credentials}'\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://marketplace.deweydata.io/api/auth/tks/get_token\", headers=headers)\n",
    "# print(response.json())\n",
    "# access_token = response.json()['access_token']\n",
    "# print(access_token)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "from getpass import getpass\n",
    "access_token = getpass(\"Enter the access token: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = r'J923tlBL.54vRQ2rAhcMc8nhb1v6IZyJwCWUkgG2LA33tHCpJJXhFwilrUhT1ckxJ'\n",
    "\n",
    "url = \"https://app.deweydata.io/external-api/v3/products/176f0262-c1f6-4dbe-be43-6a6eb21bcf8a/files\"\n",
    "url = r'https://app.deweydata.io/external-api/v2/products/c9952f25-413b-4e4a-9497-c68f01b73250/files'\n",
    "results = requests.get(url=url,\n",
    "                       headers={\n",
    "                        \"X-API-KEY\": access_token,\n",
    "                        'accept': 'application/json'\n",
    "                       })\n",
    "print(results.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.json().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.json()['metadata']\n",
    "results.json()['download_links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, link in enumerate(results.json()[\"download_links\"]):\n",
    "    print(f\"Downloading file {i}...\", link)\n",
    "    # data = requests.get(link)\n",
    "    # open(f'file-{i}.csv.gz', 'wb').write(data.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "# from posixpath import basename\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "def get_access_token(account=\"hning@email.sc.edu\", password=\"Nhwwdewey2022,\", verbose=True):\n",
    "    # un =  # Set username\n",
    "    # pw = # Set password\n",
    "\n",
    "    credentials = f\"{account}:{password}\" # Format credentials according to the API's expectations\n",
    "    # print(credentials)\n",
    "\n",
    "    credentials_bytes = credentials.encode('ascii')\n",
    "    base64_credentials_bytes = base64.b64encode(credentials_bytes)\n",
    "    base64_credentials = base64_credentials_bytes.decode('ascii')\n",
    "    # print(base64_credentials)\n",
    "\n",
    "\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Authorization': f'Basic {base64_credentials}'\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://marketplace.deweydata.io/api/auth/tks/get_token\", headers=headers)\n",
    "    \n",
    "    access_token = response.json()['access_token']\n",
    "    \n",
    "    if verbose:\n",
    "        print(response.json())\n",
    "        print(access_token)\n",
    "    \n",
    "    return access_token\n",
    "\n",
    "def safe_file_write(file_path, content):\n",
    "    # Create a temporary file in the same directory as the target file\n",
    "    dir_name = os.path.dirname(file_path)\n",
    "    with tempfile.NamedTemporaryFile(mode='wb', dir=dir_name, delete=False) as temp_file:\n",
    "        try:\n",
    "            # Write the content to the temporary file\n",
    "            temp_file.write(content)\n",
    "            temp_file.flush()\n",
    "            os.fsync(temp_file.fileno())\n",
    "            temp_file.close()\n",
    "\n",
    "            # Rename the temporary file to the target file (atomic operation)\n",
    "            os.replace(temp_file.name, file_path)\n",
    "        except Exception as e:\n",
    "            # Handle the exception (e.g., log it, raise it, or silently ignore it)\n",
    "            print(f\"An error occurred while writing the file: {e}\")\n",
    "            os.unlink(temp_file.name)\n",
    "        else:\n",
    "            print(\"File successfully written.\")\n",
    "            \n",
    "            \n",
    "# Main function\n",
    "def download_all_files(save_dir, headers, override=False):\n",
    " \n",
    "    init_url = r'/api/data/v2/list'\n",
    "    home_url = 'https://marketplace.deweydata.io'    \n",
    "    web_dir_list = [init_url]\n",
    "    \n",
    "    print(\"Started...\")\n",
    "    \n",
    "    skipped_cnt = 0\n",
    "    while len(web_dir_list) > 0:\n",
    "        try:\n",
    "            # web_dir = web_dir_list.pop()\n",
    "            # print(\"web_dir_list:\", web_dir_list)\n",
    "            web_dir = web_dir_list.pop()   # Get the latest file\n",
    "            \n",
    "            \n",
    "            url = home_url + web_dir\n",
    "            # print(url, web_dir)\n",
    "            items = requests.get(url, headers=headers).json()\n",
    "            # print(requests.get(url, headers=headers).url)\n",
    "            \n",
    "            items = items[::-1]  # Put the latest file in the end.\n",
    " \n",
    "\n",
    "            for item in items:\n",
    "                name = item['name']\n",
    "                if item['directory']:  \n",
    "                    \n",
    "                    # skip data\n",
    "                    # if r'/2021/' not in url:\n",
    "                    #     # print(\"Skipped:\", url)\n",
    "                    #     skipped_cnt += 1\n",
    "                    #     if skipped_cnt % 10 == 0:\n",
    "                    #         print(\"skipped un-targeted files:\", skipped_cnt)\n",
    "                    #     continue\n",
    "                    \n",
    "                    web_dir = item['url']\n",
    "                    \n",
    "                    full_web_dir = home_url + web_dir\n",
    "\n",
    "                    local_dir = item['parent'].replace(init_url, '').replace(r'/', '\\\\')[1:] # for windows  \n",
    "                                        \n",
    "\n",
    "                    web_dir_list.append(web_dir)\n",
    "                    print(f\"Waiting directories (count: {len(web_dir_list)}):\\n\", web_dir_list[0], '...', web_dir_list[-1])\n",
    "                    \n",
    "                    \n",
    "                    new_folder = os.path.join(save_dir, local_dir)\n",
    "                    # os.makedirs(new_folder, exist_ok=True)\n",
    "                    \n",
    "                    if not os.path.exists(new_folder):\n",
    "                         os.makedirs(new_folder, exist_ok=True)    \n",
    "                    # new_folder = os.path.join(save_dir, save_dir_name)    \n",
    "                    # os.makedirs(new_folder, exist_ok=True)\n",
    "\n",
    "                    # print(f'Creaet a folder {new_folder} for web directory: {full_web_dir}')\n",
    " \n",
    "                else:\n",
    "                    \n",
    "                    basename = item['name']     \n",
    "                    # if not 'spend' in basename:\n",
    "                    #     continue\n",
    "                    url = home_url + item['url']\n",
    "                    \n",
    "                    # skip data\n",
    "                    # if r'/ADVAN/' not in url:\n",
    "                    #     # print(\"Skipped:\", url)\n",
    "                    #     skipped_cnt += 1\n",
    "                    #     if skipped_cnt % 10 == 0:\n",
    "                    #         print(\"skipped un-targeted files:\", skipped_cnt)\n",
    "                    #     continue\n",
    "                    \n",
    "                    full_web_dir = home_url + web_dir\n",
    "                    local_dir = item['parent'].replace(init_url, '').replace(r'/', os.sep)[1:] # for windows  \n",
    "                    # print(\"save_dir, local_dir\", save_dir, local_dir, os.path.join(save_dir, local_dir))\n",
    "                    new_folder = os.path.join(save_dir, local_dir)\n",
    "                    # os.makedirs(new_folder, exist_ok=True)\n",
    "                    \n",
    "                    if not os.path.exists(new_folder):\n",
    "                         os.makedirs(new_folder, exist_ok=True)\n",
    "                    \n",
    "                    filename = os.path.join(new_folder, basename).replace(r'/', os.sep)  # for windows\n",
    "                    # print(\"new_folder, local_dir, filename, new_folder:\", new_folder, local_dir, filename, new_folder)\n",
    "                    \n",
    "                    # whether skip existing files:\n",
    "                    if not override:\n",
    "                        if os.path.exists(filename):\n",
    "                            # print(\"File exists, skipped:\", filename)\n",
    "                            skipped_cnt += 1\n",
    "                            if skipped_cnt % 50 == 0:\n",
    "                                print(\"skipped un-targeted files:\", skipped_cnt)\n",
    "                                print(\"File exists, skipped:\", filename)\n",
    "                            continue\n",
    "                            continue\n",
    "                    \n",
    "                    new_folder = os.path.dirname(filename)    \n",
    "                    os.makedirs(new_folder, exist_ok=True)\n",
    "                    \n",
    "                    print(f\"Downloading: {url}\")\n",
    "                    \n",
    "#                     with open(filename, 'wb') as f:\n",
    "                        \n",
    "#                         r = requests.get(url, stream=True, headers=headers)\n",
    "#                         f.write(r.content)\n",
    "                    r = requests.get(url, stream=True, headers=headers)\n",
    "                    safe_file_write(file_path=filename, content=r.content)\n",
    "        \n",
    "                    # with requests.get(url, stream=True, headers=headers) as r:\n",
    "                    #     # r.raise_for_status()\n",
    "                    #     with open(filename, 'wb') as f:\n",
    "                    #         for chunk in r.iter_content(chunk_size=8192):\n",
    "                    #             if chunk:\n",
    "                    #                 f.write(chunk)\n",
    "                    \n",
    "                    print(f\"Saved at  : {filename}\")\n",
    "                    \n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(url, e)\n",
    "            print(\"sleeping 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "            \n",
    "            access_token = get_access_token()\n",
    "            headers = {\n",
    "            'accept': 'application/json',\n",
    "            'Authorization': f'Bearer {access_token}'\n",
    "              }\n",
    "            download_all_files(save_dir=save_dir, headers=headers)\n",
    "            \n",
    "            continue\n",
    "            \n",
    "    print(\"Done\")\n",
    "\n",
    "# Example usage\n",
    "# access_token = r'v8-YZaYSs3t9CYkKnYywgfQ8zIs'\n",
    "\n",
    "\n",
    "access_token = get_access_token()\n",
    "\n",
    "headers = {\n",
    "            'accept': 'application/json',\n",
    "            'Authorization': f'Bearer {access_token}'\n",
    "          }\n",
    "\n",
    "save_dir = r'G:\\SafeGraph_monthly_patterns_2018-2022'\n",
    "\n",
    "download_all_files(save_dir=save_dir, headers=headers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kc6tBQL33V5u",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "b2ee03d3-b05d-4484-be1e-9e44354aafeb"
   },
   "outputs": [],
   "source": [
    "! echo -n \"hning@email.sc.edu:Nhww1898,\" | openssl base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -X 'POST' 'https://marketplace.deweydata.io/api/auth/tks/get_token' -H 'accept: application/json' -H 'Authorization: Basic LW4gImhuaW5nQGVtYWlsLnNjLmVkdTpOaHd3MTg5OCwiIA0K'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sYRsBFGX4OHj",
    "outputId": "ea1d396a-bbb0-4b61-e0e3-ce2ed7618054"
   },
   "outputs": [],
   "source": [
    "! curl -X GET \"https://marketplace.deweydata.io/api/data/v2/list\" -H \"accept: application/json\" -H \"Authorization: Bearer 0c3RBibybyGBMfXjOuCjO8qtxDA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etsryACV8f7r",
    "outputId": "1743fd4d-c01a-4c38-fc2f-362fe62e1d67"
   },
   "outputs": [],
   "source": [
    "access_token = r'0c3RBibybyGBMfXjOuCjO8qtxDA'\n",
    "# ! path = r'api/data/v2/list/2022'\n",
    "# ! curl -H 'Accept: application/json' -H \"Authorization: Bearer 0c3RBibybyGBMfXjOuCjO8qtxDA\" -X GET 'https://marketplace.deweydata.io/api/data/v2/data/2022/12/01/SAFEGRAPH/MP/20221201-safegraph_mp_home_panel_0' -o test11.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iLuhdt_m8f2n",
    "outputId": "5fa2ef4e-ed62-4b65-d062-8d7a1ad300ee"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_all_files():\n",
    "    headers = {\n",
    "            'Accept': 'application/json',\n",
    "            'Authorization': 'Bearer 0c3RBibybyGBMfXjOuCjO8qtxDA'\n",
    "            }\n",
    "    res = requests.get(\n",
    "                        'https://marketplace.deweydata.io/api/data/v2/list',                     \n",
    "                    headers=headers)  \n",
    "    \n",
    "    print(res.url)\n",
    "    return res.json()\n",
    "\n",
    "dirs = get_all_files()\n",
    "dirs\n",
    "# open('visit_panel_summary2.csv', 'wb').write(res.content)\n",
    "url = r'https://marketplace.deweydata.io/api/data/v2/data/2022/12/01/SAFEGRAPH/MP/20221201-safegraph_mp_cpgp_part8_0'\n",
    "res = requests.get(\n",
    "                    url,                     \n",
    "                    headers=headers)  \n",
    "open('visit_panel_summary2.csv', 'wb').write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "def get_filename_from_cd(cd):\n",
    "    \"\"\"\n",
    "    Get filename from content-disposition\n",
    "    \"\"\"\n",
    "    if not cd:\n",
    "        return None\n",
    "    fname = re.findall('filename=(.+)', cd)\n",
    "    print(fname)\n",
    "    if len(fname) == 0:\n",
    "        return None\n",
    "    return fname[0]\n",
    "\n",
    "\n",
    "url = r'https://marketplace.deweydata.io/api/data/v2/data/2022/12/01/SAFEGRAPH/MP/20221201-safegraph_mp_cpgp_part8_0'\n",
    "\n",
    "# url = 'http://google.com/favicon.ico'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "filename = get_filename_from_cd(r.headers.get('content-disposition'))\n",
    "open(filename, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from posixpath import basename\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Main function\n",
    "def download_all_files(save_dir, headers):\n",
    " \n",
    "    init_url = r'/api/data/v2/list'\n",
    "    home_url = 'https://marketplace.deweydata.io'    \n",
    "    web_dir_list = [init_url]\n",
    "    \n",
    "    while len(web_dir_list) > 0:\n",
    "        try:\n",
    "            web_dir = web_dir_list.pop()\n",
    "            \n",
    "            \n",
    "            url = home_url + web_dir\n",
    "            # print(url, web_dir)\n",
    "            items = requests.get(url, headers=headers).json()\n",
    "            \n",
    "            items = items[::-1]\n",
    "            \n",
    "\n",
    "            for item in items:\n",
    "                name = item['name']\n",
    "                if item['directory']:  \n",
    "                    \n",
    "                    web_dir = item['url']\n",
    "                    \n",
    "                    full_web_dir = home_url + web_dir\n",
    "\n",
    "                    save_dir_name = full_web_dir.replace(home_url + init_url + '/', \"\").replace(r'/', '\\\\')  # for windows\n",
    "\n",
    "                    new_folder = os.path.join(save_dir, save_dir_name)    \n",
    "                    os.makedirs(new_folder, exist_ok=True)\n",
    "\n",
    "                    web_dir_list.append(web_dir)    \n",
    "\n",
    "                    print(f'Creaet a folder {new_folder} for web directory: {full_web_dir}')\n",
    " \n",
    "                else:\n",
    "                    \n",
    "                    basename = item['name']     \n",
    "                    # if not 'spend' in basename:\n",
    "                    #     continue\n",
    "                    url = home_url + item['url']\n",
    "                    full_web_dir = home_url + web_dir\n",
    "                    local_dir = item['parent'].replace(init_url, '').replace(r'/', '\\\\')[1:] # for windows  \n",
    "                    # print(\"save_dir, local_dir\", save_dir, local_dir, os.path.join(save_dir, local_dir))\n",
    "                    new_folder = os.path.join(save_dir, local_dir)\n",
    "                    os.makedirs(new_folder, exist_ok=True)\n",
    "                    \n",
    "                    filename = os.path.join(new_folder, basename).replace(r'/', '\\\\')  # for windows\n",
    "                    # print(\"new_folder, local_dir, filename, new_folder:\", new_folder, local_dir, filename, new_folder)\n",
    "                    with requests.get(url, stream=True, headers=headers) as r:\n",
    "                        # r.raise_for_status()\n",
    "                        with open(filename, 'wb') as f:\n",
    "                            for chunk in r.iter_content(chunk_size=8192):\n",
    "                                if chunk:\n",
    "                                    f.write(chunk)\n",
    "                    print(f\"Downloaded: {url}\")\n",
    "                    print(f\"Saved at  : {filename}\")\n",
    "                    \n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(url, e)\n",
    "            print(\"sleeping 6 seconds...\")\n",
    "            time.sleep(6)\n",
    "            \n",
    "            continue\n",
    "\n",
    "# Example usage\n",
    "access_token = r'v8-YZaYSs3t9CYkKnYywgfQ8zIs'\n",
    "headers = {\n",
    "            'accept': 'application/json',\n",
    "            'Authorization': f'Bearer {access_token}'\n",
    "          }\n",
    "\n",
    "save_dir = r'L:\\SafeGraph_monthly_patterns_2018-2022'\n",
    "\n",
    "download_all_files(save_dir=save_dir, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "s6yJsmmbXYhM",
    "outputId": "d0a6b043-cfa7-482b-b3e3-9b7fcaa448e0"
   },
   "outputs": [],
   "source": [
    "# from posixpath import basename\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Function to download a file\n",
    "def download_file(home_url, item, save_folder, headers):\n",
    "\n",
    "    basename = item['name']\n",
    "    \n",
    "    \n",
    "    \n",
    "    url = home_url + item['url']\n",
    "    new_folder = os.path.join(save_folder, url.replace(home_url + '/', ''))\n",
    "    os.makedirs(new_folder, exist_ok=True)\n",
    "    \n",
    "    filename = os.path.join(new_folder, basename)\n",
    "    \n",
    "    with requests.get(url, stream=True, headers=headers) as r:\n",
    "        # r.raise_for_status()\n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    print(f\"Downloaded: {url}\")\n",
    "    print(f\"Saved at  : {filename}\")\n",
    "\n",
    "# Function to download a directory recursively\n",
    "def download_directory(directory, save_folder, headers):\n",
    "    home_url = 'https://marketplace.deweydata.io'\n",
    "    # print(\"directory: \", directory)\n",
    "    url = home_url + directory\n",
    "    # print(\"url:\", url)\n",
    "    items = requests.get(url, headers=headers).json()\n",
    "    \n",
    "    # print(\"headers:\", headers)\n",
    "    # print(url, items)\n",
    "\n",
    "    for item in items:\n",
    "        name = item['name']\n",
    "        # print(item)\n",
    "        # item_path = \n",
    "        \n",
    "        if item['directory']:\n",
    "            save_dir = os.path.join(save_folder, name)\n",
    "            # directory = home_url + \n",
    "            directory = item['url']\n",
    "            # print(\"directory: \", directory)\n",
    "            new_folder = os.path.join(save_folder, directory.replace(home_url, ''))\n",
    "            \n",
    "            os.makedirs(new_folder, exist_ok=True)\n",
    "            download_directory(directory, save_folder, headers=headers)\n",
    "        else:\n",
    "            download_file(home_url, item, save_folder, headers=headers)\n",
    "\n",
    "# Main function\n",
    "def download_all_files(save_folder):\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Authorization': 'Bearer 0c3RBibybyGBMfXjOuCjO8qtxDA'\n",
    "    }\n",
    "\n",
    "\n",
    "    download_directory(directory='/api/data/v2/list', save_folder=save_folder, headers=headers)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "save_dir = r'L:\\SafeGraph_monthly_patterns_2018-2022'\n",
    "download_all_files(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vrSNCOev8fvn",
    "outputId": "a8d8bfc2-1a9f-4227-dfd2-3afb031cff6c"
   },
   "outputs": [],
   "source": [
    "# files = res.json()\n",
    "\n",
    "type(files)\n",
    "files[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "id": "aeLoesOXMwxO",
    "outputId": "04ba75c7-5af0-4e75-ea92-e417e73267f8"
   },
   "outputs": [],
   "source": [
    "curl -H \"Accept: application/json\" -H \"Authorization: Bearer 0c3RBibybyGBMfXjOuCjO8qtxDA\" -X GET \"/api/data/v2/list/2022/12/01/SAFEGRAPH/MP/2022/12/01/SAFEGRAPH/MP/20221201-safegraph_mp_home_panel_0\" -o test42.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge panel CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_dir = r'L:\\SafeGraph_monthly_patterns_2018-2022'\n",
    "\n",
    "\n",
    "def get_all_files(data_dir, exts=['.csv'], verbose=True):\n",
    "    all_files = []\n",
    "    for root_dir, folders, files in os.walk(data_dir):\n",
    "        \n",
    "        for f in files:\n",
    "            for ext in exts:\n",
    "                ext_len = len(ext)\n",
    "                if ext == f[-ext_len:]:\n",
    "                    full_name = os.path.join(root_dir, f)\n",
    "                    all_files.append(full_name)\n",
    "    if verbose:\n",
    "        print(\"Found files:\", len(all_files))\n",
    "    return sorted(all_files)\n",
    "\n",
    "\n",
    "get_all_files(data_dir, exts=['home_panel_summary.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2018, 2019, 2020, 2021]\n",
    "\n",
    "def merge_CSV_files():\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "wildfire",
   "language": "python",
   "name": "wildfire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
